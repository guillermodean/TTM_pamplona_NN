{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing and predicting nok torques given by electronic screwdrivers in a seat factory.\n",
    "---\n",
    "## Author: Guillermo Dean\n",
    "\n",
    "\n",
    "### **Interpreter:** conda env with python 3.8.8\n",
    "### **Data:** Isringhausen Spain SLU 2 months of torque results from  CVINET\n",
    "\n",
    "---\n",
    "\n",
    "#### Needed Nvidia CUDA  [CuDNN](https://www.tensorflow.org/install/gpu?hl=es-419) and all intallation on the link\n",
    "#### to install with pip tensorflow i had to change the registry value for longpaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import   LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing and exploration\n",
    "\n",
    "Let's read the data set that is stored in data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('data/Results_TTM.csv',sep=\";\",header=0)\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove all the columns with all null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns=[\"Step status\",\"Current trend\",\"Torque rate min\",\"Torque rate max\",\"Torque rate trend\",\"CVILOGIX\",\"Identifier6\",\"Identifier7\",\"Identifier8\",\"Identifier9\",\"Identifier10\",\"Second transducer torque deviation\",\"Second transducer angle deviation\",\"Result type\",\"Pulse counter\",\"Angle offset\",\"AO torque rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.describe)\n",
    "df=df[['Result status','Result number','Time result','Pset ID','Step ID','Error Code', 'Torque min','Torque','Torque max','Angle min','Angle','Angle max','Pset name','VIN','Identifier1','Identifier2','Identifier3','Identifier4','Identifier5']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are left with the columns that have interesting values and we remove all the columns that contain many nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rename the columns to eliminate the spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.rename(columns={\"Result status\":\"Result_status\",\"Result number\":\"Result_number\",\"Pset ID\":\"Pset_ID\",\"Step ID\":\"Step_ID\",\"Torque min\":\"Torque_min\",\"Torque max\":\"Torque_max\",\"Angle min\":\"Angle_min\",\"Angle max\":\"Angle_max\",\"Pset name\":\"Pset_name\",'Error code':'Error_code','Result_status':'Result_status','Time result':'Time_result'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to see which of the torques are NOK, we filter the result column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nok=df[df['Result_status'].str.contains(\"NOK\")]\n",
    "df_nok.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 22568 NOK results but lets see which of the Pset programmed are the ones which fails the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nok['Pset_name'].head()\n",
    "df_nok[['Pset_name','Result_status']].groupby(by='Pset_name').count().sort_values(by=['Result_status'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The worst pset is seat fram front_35Nm with 1109 nok results\n",
    "We see also that there is a POKA YOKE result, it is a forced NOK result that is done to check the tools. they must be removed from the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df[\"Pset_name\"].str.contains(\"Poka\",na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We change the type of the Time_result column to make it match the date type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.astype({'Time_result': 'datetime64[ns, US/Eastern]'}).dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new column the result column that contains the texts \"NOK\" and \"OK\" in numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Resultbin']=df['Result_status']=='OK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OKS = len(df[df[\"Result_status\"].str.contains(\"OK\",na=False)])\n",
    "NOKS=len(df[df[\"Result_status\"].str.contains(\"NOK\",na=False)])\n",
    "result=NOKS/OKS\n",
    "print(\"NOK percentage: {:2.2%}\".format(result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 5% bad torques in the whole data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is biased we have lots of OK torques against a few of NOK torques which is good for the company but not for our puroposes.\n",
    "with the code below we will plot the difference between results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX_plot=df[['Result_status','Step_ID']].groupby(by='Result_status').count()\n",
    "ax = dfX_plot.plot.bar(y='Step_ID',rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have columns that are not going to contribute anything to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX=df.drop(columns={'Result_status','Result_number','Time_result','Pset_ID','Identifier4','Identifier5','Identifier2','Error Code','Torque','Angle'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with df.info () we are going to see how many columns contain null results and we are going to make the whole sample have columns with data that we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the values that we are going to remove. because the sample is going to be reduced a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to build a new colum called value_is_NAN which will contain yes or nos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_NANs=dfX[['Pset_name','Identifier1','Identifier3','Resultbin']]\n",
    "df_NANs.loc[df_NANs['Identifier1'].isnull(),'value_is_NaN'] = 'Yes'\n",
    "df_NANs.loc[df_NANs['Identifier1'].notnull(), 'value_is_NaN'] = 'No'\n",
    "df_NANs = df_NANs[df_NANs[\"value_is_NaN\"].str.contains(\"Yes\",na=False)]\n",
    "df_NANs[['Pset_name','Resultbin']].groupby(by='Pset_name').count().sort_values(by='Resultbin',ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are losing OK and NOK values from the NTS1 and NTS2 lines that do not store valueIdentifiers. In other words, the analysis will focus on the lines:\n",
    "1. ALter BAU\n",
    "2. Alter BUS\n",
    "3. Tapizado NTS1\n",
    "4. Tapizado NTS2\n",
    "5. NTS1 BAU\n",
    "We are going to proceed to eliminate the null results of the Df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX=dfX.dropna()\n",
    "dfX = dfX[pd.to_numeric(dfX['Identifier3'],errors='coerce').notna()]\n",
    "dfX.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now observe that we have 168,002 non-null records with which we can build a model. let's see the percentage of total NOK tightening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX[['Resultbin','Step_ID']].groupby(by='Resultbin').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets check again how many NOK against Ok torques we have in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OKS = len(df[df[\"Resultbin\"]==True])\n",
    "NOKS=len(df[df[\"Resultbin\"]==False])\n",
    "result=NOKS/OKS\n",
    "print(\"NOK percentage: {:2.2%}\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% of NOKs has increased but not much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX_plot=dfX[['Resultbin','Step_ID']].groupby(by='Resultbin').count()\n",
    "ax = dfX_plot.plot.bar(y='Step_ID',rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unbias = dfX.drop(dfX[dfX['Resultbin'] == True].sample(frac=.1, random_state=101).index)\n",
    "train_labels = np.array(dfX.pop('Resultbin'))\n",
    "bool_train_labels = train_labels != 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will have the posibility of increas NOK values in our dataset by randomly removing 10% OK values from resultbin. lets see an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OKS = len(df_unbias[df_unbias[\"Resultbin\"]==True])\n",
    "NOKS=len(df_unbias[df_unbias[\"Resultbin\"]==False])\n",
    "result=NOKS/OKS\n",
    "print(\"NOK percentage: {:2.2%}\".format(result))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be tuning this percentage during the model analisis to see if increasing or decreasing it will improve the predictions and metrics of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of the data set for treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see we have labeled values (identifiers, pset...). In order for our model to work we need to convert this labels to a numerical value.\n",
    "For that we will use label encoder from the sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "enc=LabelEncoder()\n",
    "dfX=df[['Torque_min','Torque_max','Angle_min','Angle_max','Pset_name',\t'Identifier1','Identifier3','Resultbin']].dropna()\n",
    "dfX = dfX.drop(dfX[dfX['Resultbin'] == True].sample(frac=.1, random_state=101).index)\n",
    "#Quito el 40 % de los resultados OK  de DFX\n",
    "\n",
    "dfX['Identifier3'] = df['Identifier3'].astype('string',copy=False)\n",
    "dfX['Pset_name_cat'] = enc.fit_transform(dfX['Pset_name'])\n",
    "dfX['Modelo'] = enc.fit_transform(dfX['Identifier1'])\n",
    "dfX[['Identifier3','Resultbin']].groupby(by=\"Identifier3\").count()\n",
    "dfX[dfX['Identifier3'].apply(lambda x: x.isnumeric())]\n",
    "dfX['Trabajador'] = enc.fit_transform(dfX['Identifier3'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX[['Identifier3','Resultbin']].groupby(by=\"Identifier3\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see the number of OK and NOK results we have => it will be usefull later to compare the predictions of the model with the real values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX_gp=dfX[['Resultbin','Pset_name_cat']].groupby(by='Resultbin').count()\n",
    "dfX_gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OKS = dfX_gp.iloc[0].values\n",
    "NOKS = dfX_gp.iloc[1].values\n",
    "print(\"OKS\"+str(OKS)+\" and NOKS \"+str(NOKS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOKS/OKS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we are going to leave the df with only the columns we are going to use in the model and we will start preparing our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX[\"Resultbin\"] = dfX[\"Resultbin\"].astype(int)\n",
    "y=np.array(dfX[\"Resultbin\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX_pairplot=dfX.drop(columns={'Pset_name','Identifier1','Identifier3'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.pairplot (dfX_pairplot,hue='Resultbin') #Takes 5 minutes to plot all features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see again the imbalanced data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX=dfX.drop(columns={'Resultbin','Pset_name','Identifier1','Identifier3'})\n",
    "X=dfX.values\n",
    "print(y, X)\n",
    "dfX\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=[]\n",
    "for col in dfX:\n",
    "    columns.append(col)\n",
    "print (columns)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see on the cell up that we now have one array with the results and other one with the labels already encoded as numbers.\n",
    "Lets have an overview on the final data: the code below displays all the features an the correlation between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to try to predict OK and NOK results with a neural network model. In other notebook I have already tried to use other ML learning models you can check it in my github repos.\n",
    "we have to import now all the packages necesaries to build the model (tensorflow, sklearn).\n",
    "We are also configuring the notebook to run the on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy, BinaryCrossentropy\n",
    "from sklearn.utils import shuffle\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs available:\", len(physical_devices))\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "label=np.array(y)\n",
    "sample=np.array(X)\n",
    "\n",
    "print(label.shape,sample.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks preform better when the input data is in a 0 to 1 range so we will use min-max-Scaler to scale our labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label,sample =shuffle(label,sample)\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_samples= scaler.fit_transform(sample) #fit transform does not accept 1D data so we reshape the scaled train samples to be 2D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now check the shape of our arrays X and y before an after scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y, X)\n",
    "print(y.shape,X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label,scaled_samples)\n",
    "print(label.shape,scaled_samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are now going to use train_test_split to split arrays or matrices into random train and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_samples, label, test_size=0.1, random_state=101)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape, X_train.shape)\n",
    "print(y_test.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding useful metrics\n",
    "Notice that there are a few metrics defined above that can be computed by the model that will be helpful when evaluating the performance.\n",
    "\n",
    "* **False negatives** and false positives are samples that were incorrectly classified\n",
    "* **True negatives** and true positives are samples that were correctly classified\n",
    "* **Accuracy** is the percentage of examples correctly classified > \n",
    "* **Precision** is the percentage of predicted positives that were correctly classified > \n",
    "* **Recall** is the percentage of actual positives that were correctly classified > \n",
    "* **AUC** refers to the Area Under the Curve of a Receiver Operating Characteristic curve (ROC-AUC). This metric is equal to the probability that a classifier will rank a random positive sample higher than a random negative sample.\n",
    "* **AUPRC** refers to Area Under the Curve of the Precision-Recall Curve. This metric computes precision-recall pairs for different probability thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      tf.keras.metrics.TruePositives(name='tp'),\n",
    "      tf.keras.metrics.FalsePositives(name='fp'),\n",
    "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "      tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall'),\n",
    "      tf.keras.metrics.AUC(name='auc'),\n",
    "      tf.keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Comprender métricas útiles\n",
    "\n",
    "Tenga en cuenta que hay algunas métricas definidas anteriormente que pueden ser calculadas por el modelo y que serán útiles al evaluar el desempeño.\n",
    "\n",
    "* Los falsos negativos y falsos positivos son muestras que fueron clasificadas incorrectamente.\n",
    "\n",
    "* Verdaderos negativos y positivos verdaderos son muestras que fueron clasificados correctamente.\n",
    "\n",
    "* La precisión es el porcentaje de ejemplos correctamente clasificada.\n",
    "\n",
    "* La precisión es el porcentaje de positivos predichos que se clasifican correctamente.\n",
    "\n",
    "* Recall es el porcentaje de positivos reales que fueron clasificados correctamente.\n",
    "\n",
    "* AUC se refiere al área bajo la curva de una curva característica de funcionamiento del receptor (ROC-AUC). Esta métrica es igual a la probabilidad de que un clasificador clasifique una muestra positiva aleatoria por encima de una muestra negativa aleatoria.\n",
    "\n",
    "* AUPRC se refiere al área bajo la curva de la curva de precisión de recordar. Esta métrica calcula pares de recuperación de precisión para diferentes umbrales de probabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Tune model with keras tuner\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are going now to create the models using the functions make_model, model_builder and super model\n",
    "What i have done here is to join two models:\n",
    "* The make model is a clasification model with one last layer containing to nodes and that is evaluated with binary_crossentropy and that I used it to tune initial bias and initial weights.\n",
    "* The model_builder is another model which I used to autotune hyperparameters with hyperband (This algorithm is one of the tuners available in the keras-tuner library):\n",
    "    * number of units per layer\n",
    "    * best epochs\n",
    "    * best learning rate\n",
    "Hyperband randomly sample all the combinations of hyperparameter and now instead of running full training and evaluation on it, train the model for few epochs (less than max_epochs) with these combination => its like a champions league tournament the best results advance in the competition.\n",
    "* The third model is the supermodel which is a combination of the two previous models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 2048\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_prc', \n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the model is fit using a larger than default batch size of 2048, this is important to ensure that each batch has a decent chance of containing a few positive samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(metrics=METRICS,output_bias=None):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(units=16,input_shape=[len(dfX.keys())],activation='relu'),\n",
    "        Dense(units=32,activation='relu'),\n",
    "        Dense(units=32,activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(units=2,activation='sigmoid',bias_initializer=output_bias)  # dos clases, par ok par nok.\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001),loss='BinaryCrossentropy',metrics=metrics)\n",
    "    \n",
    "    # voy a cambiar el optimizer a ver si mejora \n",
    "    # from tensorflow.keras.optimizers import SGD\n",
    "    # from tensorflow.keras.metrics import categorical_crossentropy\n",
    "    # opt = SGD(learning_rate=0.01)\n",
    "    # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt, metrics= ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Tune the number of units in the first Dense layer\n",
    "    # Choose an optimal value between 32-512\n",
    "\n",
    "    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(units=16, input_shape=[\n",
    "              len(dfX.keys())], activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(units=hp_units, activation='relu'))\n",
    "    # added one layer more\n",
    "    model.add(tf.keras.layers.Dense(units=hp_units, activation='relu'))\n",
    "    # changed to sigmoid => is equivalent to softmax for two outputs\n",
    "    model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    # Tune the learning rate for the optimizer\n",
    "    # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                      from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_model(hp, metrics=METRICS,output_bias=None):\n",
    "    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(units=hp_units, input_shape=[\n",
    "              len(dfX.keys())], activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(units=(2*hp_units)/3, activation='relu'))\n",
    "    # added one layer more\n",
    "    model.add(tf.keras.layers.Dense(units=hp_units/3, activation='relu'))\n",
    "    # changed to sigmoid => is equivalent to softmax for two outputs\n",
    "    model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    # Tune the learning rate for the optimizer\n",
    "    # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                  loss='BinaryCrossentropy',\n",
    "                  metrics=metrics)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the tuner and we save the results in a folder in our src called my_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(super_model,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=10,\n",
    "                     factor=3,\n",
    "                     directory='my_dir',\n",
    "                     project_name='NN_TTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(X_train, y_train, epochs=50,batch_size=BATCH_SIZE , validation_split=0.2, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the model once we have decided the optimal number of units of the first layer:\n",
    "I have used  one of the few rules of thumb that it can be used to define hidden layers unit number:\n",
    "*The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer*\n",
    "I have also defined 2 as the number of hidden layers. This way it can represent an arbitrary decision boundary to arbitrary accuracy with rational activation functions and can approximate any smooth mapping to any accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=super_model(hp=best_hps)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets evaluate the model without initial bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_train, y_train, batch_size=BATCH_SIZE, verbose=1)\n",
    "print(\"Loss: {:0.4f}\".format(results[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct bias to set can be derived from: what we did here is the log of 7 times the ok size to improve the loss metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_bias=np.log([7*OKS[0]/NOKS[0]])\n",
    "initial_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chack again the model but now with the initial bias defined previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = super_model(hp=best_hps,output_bias=initial_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_train, y_train, batch_size=BATCH_SIZE, verbose=2)\n",
    "print(\"Lossca: {:0.4f}\".format(results[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its aproximatedly 5% better result than the previous one, This way the model doesn't need to spend the first few epochs just learning that positive examples are unlikely. This also makes it easier to read plots of the loss during training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_bias=None\n",
    "# results = model.evaluate(X_train, y_train, batch_size=10, verbose=0)\n",
    "# print(\"Loss: {:0.4f}\".format(results[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now defining the starting control point for the weights and we are going to store them on a temp folder called initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "initial_weights = os.path.join(tempfile.mkdtemp(), 'initial_weights')\n",
    "model.save_weights(initial_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that the bias fix helps. we start training the model with zero bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = super_model(hp=best_hps)\n",
    "model.load_weights(initial_weights)\n",
    "model.layers[-1].bias.assign([0.0])\n",
    "zero_bias_history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=2048,\n",
    "    epochs=20,\n",
    "    validation_split=0.1, \n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train again the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = super_model(hp=best_hps,output_bias=initial_bias)\n",
    "model.load_weights(initial_weights)\n",
    "careful_bias_history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=2048,\n",
    "    epochs=20,\n",
    "    validation_split=0.1, \n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to import matplotlib and we are going to define the size of the plot, also the colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below it is described the plot loss function to plot loss and validation-loss improvement during the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history, label, n):\n",
    "  # Use a log scale on y-axis to show the wide range of values.\n",
    "  plt.semilogy(history.epoch, history.history['loss'],\n",
    "               color=colors[n], label='Train ' + label)\n",
    "  plt.semilogy(history.epoch, history.history['val_loss'],\n",
    "               color=colors[n], label='Val ' + label,\n",
    "               linestyle=\"--\")\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets check first the numerical values of the loss for the zero bias history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_bias_history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are plotting boz zero bias and carefull bias to compare if there has been any improvements with this action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(zero_bias_history, \"Zero Bias\", 0)\n",
    "plot_loss(careful_bias_history, \"Careful Bias\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it looks like it starts better but they tend to the same value both zero bias and carefull bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets tune now the best epochs:\n",
    "1. first of all I am going to train the model with 100 epochs to se where the early_stopping will stop the training.\n",
    "2. then i will pass this value to the hyperband tuner with the hyperband tuner training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = super_model(hp=best_hps,output_bias=initial_bias)\n",
    "model.load_weights(initial_weights)\n",
    "baseline_history=model.fit(x=X_train,y=y_train, validation_split=0.1,batch_size=BATCH_SIZE,epochs=100,callbacks=[early_stopping] , verbose=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Best val_loss and loss: {:0.4f} '.format(min(baseline_history.history['val_loss'],)))\n",
    "print('Best loss: {:0.4f}'.format(min(baseline_history.history['loss'],)))\n",
    "val_acc_per_epoch = baseline_history.history['val_loss']\n",
    "best_epoch = val_acc_per_epoch.index(min(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: {:d}'.format(best_epoch,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we pass best epoch to the hpyermodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "\n",
    "model = tuner.hypermodel.build(best_hps,output_bias=initial_bias)\n",
    "model.load_weights(initial_weights)\n",
    "history = model.fit(X_train, y_train, epochs=best_epoch,batch_size=BATCH_SIZE, validation_split=0.2,verbose=0)\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history):\n",
    "  metrics = ['loss', 'prc', 'precision', 'recall']\n",
    "  for n, metric in enumerate(metrics):\n",
    "    name = metric.replace(\"_\",\" \").capitalize()\n",
    "    plt.subplot(2,2,n+1)\n",
    "    plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n",
    "    plt.plot(history.epoch, history.history['val_'+metric],\n",
    "             color=colors[0], linestyle=\"--\", label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(name)\n",
    "    if metric == 'loss':\n",
    "      plt.ylim([0, plt.ylim()[1]])\n",
    "    elif metric == 'auc':\n",
    "      plt.ylim([0.8,1])\n",
    "    else:\n",
    "      plt.ylim([0,1])\n",
    "\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(baseline_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions and confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicciones del modelo con pesos y sesgo de partida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_baseline = model.predict(X_train, batch_size=BATCH_SIZE)\n",
    "test_predictions_baseline = model.predict(X_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicciones del modelo incial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precictions=model.predict(x=X_test.tolist(),batch_size=10,verbose=0)\n",
    "print(precictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_predictions=np.argmax(precictions,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import  itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix',\n",
    "                        cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ploteo la confusion matrix para ver que tal ha aprendido mi modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_new = confusion_matrix(y_true=y_train,y_pred=train_predictions_baseline>0.5)\n",
    "cm_plot_labels=['no OK','OK']\n",
    "plot_confusion_matrix (cm_new,cm_plot_labels,title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('NOK torques detected (True Negatives): ', cm_new[0][0])\n",
    "print('NOK torques Incorrectly Detected (False Positives): ', cm_new[0][1])\n",
    "print('OK torques Missed (False Negatives): ', cm_new[1][0])\n",
    "print('OK torques Detected (True Positives): ', cm_new[1][1])\n",
    "print('Total NOK torques: ', np.sum(cm_new[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(name, labels, predictions, **kwargs):\n",
    "  fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
    "\n",
    "  plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
    "  plt.xlabel('False positives [%]')\n",
    "  plt.ylabel('True positives [%]')\n",
    "  plt.xlim([-0.5,100])\n",
    "  plt.ylim([0,100.5])\n",
    "  plt.grid(True)\n",
    "  ax = plt.gca()\n",
    "  ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(\"Train Baseline\", y_train, train_predictions_baseline, color=colors[0])\n",
    "plot_roc(\"Test Baseline\", y_test, test_predictions_baseline, color=colors[0], linestyle='--')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Plot the AUPRC\n",
    "\n",
    "\n",
    "Now plot the AUPRC. Area under the interpolated precision-recall curve, obtained by plotting (recall, precision) points for different values of the classification threshold. Depending on how it's calculated, PR AUC may be equivalent to the average precision of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prc(name, labels, predictions, **kwargs):\n",
    "    precision, recall, _ = sklearn.metrics.precision_recall_curve(labels, predictions)\n",
    "\n",
    "    plt.plot(precision, recall, label=name, linewidth=2, **kwargs)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.xlim([0,1.1])\n",
    "    plt.ylim([0,1.1])\n",
    "    plt.grid(True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prc(\"Train Baseline\", y_train, train_predictions_baseline, color=colors[0])\n",
    "plot_prc(\"Test Baseline\", y_test, test_predictions_baseline, color=colors[0], linestyle='--')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "total=OKS[0]+NOKS[0]\n",
    "weight_for_0 = (1 / NOKS[0]) * (total / 2.0)\n",
    "weight_for_1 = (1 / OKS[0]) * (total / 2.0)\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_model = super_model(hp=best_hps)\n",
    "weighted_model.load_weights(initial_weights)\n",
    "\n",
    "weighted_history = weighted_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=best_epoch,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_split=0.2 ,\n",
    "    # The class weights go here\n",
    "    class_weight=class_weight,\n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_model.layers[-1].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(weighted_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_weighted = weighted_model.predict(X_train, batch_size=BATCH_SIZE)\n",
    "test_predictions_weighted = weighted_model.predict(X_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_new = confusion_matrix(y_true=y_test,y_pred=test_predictions_weighted>0.5)\n",
    "cm_plot_labels=['no OK','OK']\n",
    "plot_confusion_matrix (cm_new,cm_plot_labels,title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(\"Train Baseline\", y_train, train_predictions_baseline, color=colors[0])\n",
    "plot_roc(\"Test Baseline\", y_test, test_predictions_baseline, color=colors[0], linestyle='--')\n",
    "\n",
    "plot_roc(\"Train Weighted\", y_train, train_predictions_weighted, color=colors[1])\n",
    "plot_roc(\"Test Weighted\", y_test, test_predictions_weighted, color=colors[1], linestyle='--')\n",
    "\n",
    "\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot AUPRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prc(\"Train Baseline\", y_train, train_predictions_baseline, color=colors[0])\n",
    "plot_prc(\"Test Baseline\", y_test, test_predictions_baseline, color=colors[0], linestyle='--')\n",
    "\n",
    "plot_prc(\"Train Weighted\", y_train, train_predictions_weighted, color=colors[1])\n",
    "plot_prc(\"Test Weighted\", y_test, test_predictions_weighted, color=colors[1], linestyle='--')\n",
    "\n",
    "\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversample the minority class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A related approach would be to resample the dataset by oversampling the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = RandomOverSampler()\n",
    "X_resampled, y_resampled = oversample.fit_resample(X_train, y_train)\n",
    "colors = ['#ef8a62' if v == 0 else '#f7f7f7' if v == 1 else '#67a9cf' for v in y_resampled]\n",
    "plt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=colors, linewidth=0.5, edgecolor='black')\n",
    "sns.despine()\n",
    "plt.title(\"RandomOverSampler Output ($n_{class}=4700)$\")\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resampled=np.column_stack((X_resampled,y_resampled)) \n",
    "df_resampled.shape\n",
    "columns.append('Resultbin')\n",
    "print(columns)\n",
    "df_resampled=pd.DataFrame(df_resampled,columns=columns)\n",
    "df_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resampled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_resampled,hue='Resultbin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have balanced our datased as can be seen in the previous graph.\n",
    "lets train again our model to see if we can improve recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have to tune again our hyperparameters with the resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(X_resampled, y_resampled, epochs=50,batch_size=BATCH_SIZE , validation_split=0.2, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we are going to recalculate the initial bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OKS = len(df_resampled[df_resampled[\"Resultbin\"]==1])\n",
    "NOKS=len(df_resampled[df_resampled[\"Resultbin\"]==0])\n",
    "result=NOKS/(OKS+NOKS)\n",
    "print(\"NOK percentage: {:2.2%}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_bias=np.log([OKS/NOKS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again the classweights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eight_for_0 = (1 / NOKS) * (total / 2.0)\n",
    "weight_for_1 = (1 / OKS) * (total / 2.0)\n",
    "\n",
    "class_weight_balanced = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_model = super_model(hp=best_hps,output_bias=initial_bias)\n",
    "weighted_model.load_weights(initial_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_balanced_history = weighted_model.fit(\n",
    "    X_resampled,\n",
    "    y_resampled,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=best_epoch,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_split=0.2 ,\n",
    "    # The class weights go here\n",
    "    class_weight=class_weight,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "plot_metrics(weighted_balanced_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_weighted_balanced = weighted_model.predict(X_resampled, batch_size=BATCH_SIZE)\n",
    "test_predictions_weighted = weighted_model.predict(X_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_new = confusion_matrix(y_true=y_test,y_pred=test_predictions_weighted>0.5)\n",
    "cm_plot_labels=['no OK','OK']\n",
    "plot_confusion_matrix (cm_new,cm_plot_labels,title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el modelo había predicho todo a la perfección, esto sería una matriz diagonal donde los valores fuera de la diagonal principal, lo que indica predicciones incorrectas, sería cero. En este caso, la matriz muestra que tiene relativamente pocos falsos positivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Me pasa lo mismo que con el modelo de regresion lineal => hay que tratar los datos para incrementar el numero de resultados NOK porcentualmente sobre el total de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confusion matrix logistic regresion in TTM_Pamplona\n",
    "\n",
    "[   12,  2308]\n",
    "\n",
    "\n",
    "[    0, 39686]\n",
    "con todo el dataframe  Neural Network % nok predicted good = 0,005\n",
    "tras reducir el número de pares Ok un 40% me sale la siguiente cm: Neural Network % nok predicted good 0.027\n",
    "\n",
    "[   80  2905]\n",
    "\n",
    "\n",
    "[    0 52462]\n",
    "\n",
    "quitando el angulo minimo. Neural Network % nok predicted good: 0.017\n",
    "\n",
    "[   53  3039]\n",
    "\n",
    "\n",
    "[    0 52355]\n",
    "\n",
    "vamos a volver a añadir  los pares ok a ver que hace but still i am too far\n",
    "\n",
    "con el 60% de los pares ok quitados: Neural Network % nok predicted NOK: 0.016\n",
    "\n",
    "[   48  3026]\n",
    "\n",
    "\n",
    "[    0 52373]\n",
    "\n",
    "vuelvo a poner el angulo mímimo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porcentaje_pares_LR=12/(12+2308)\n",
    "porcentaje_pares_NN=cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "\n",
    "print(\"Logistic regresion % nok predicted as NOK: \"+str(round(porcentaje_pares_LR,3)))\n",
    "print(\"Neural Network % nok predicted NOK: \"+str(round(porcentaje_pares_NN,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Use weight regularization.** It tries to keep weights low which very often leads to better generalization. Experiment with different regularization coefficients. Try 0.1, 0.01, 0.001 and see what impact they have on accuracy.\n",
    "\n",
    "\n",
    "2. **Corrupt your input** (e.g., randomly substitute some pixels with black or white). This way you remove information from your input and 'force' the network to pick up on important general features. Experiment with noising coefficients which determines how much of your input should be corrupted. Research shows that anything in the range of 15% - 45% works well.\n",
    "\n",
    "\n",
    "3. **Expand your training set.** Since you're dealing with images you can expand your set by rotating / scaling etc. your existing images (as suggested). You could also experiment with pre-processing your images (e.g., mapping them to black and white, grayscale etc. but the effectiveness of this technique will depend on your exact images and classes)\n",
    "\n",
    "\n",
    "4. **Pre-train your layers with denoising critera.** Here you pre-train each layer of your network individually before fine tuning the entire network. Pre-training 'forces' layers to pick up on important general features that are useful for reconstructing the input signal. Look into auto-encoders for example (they've been applied to image classification in the past).\n",
    "\n",
    "\n",
    "5. **Experiment with network architecture.** Your network might not have sufficient learning capacity. Experiment with different neuron types, number of layers, and number of hidden neurons. Make sure to try compressing architectures (less neurons than inputs) and sparse architectures (more neurons than inputs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. reducir aun mas los ok => cuanto mas?\n",
    "\n",
    "2. meter mas layers => done\n",
    "\n",
    "3. cambiar de softmax a sigmoid => done\n",
    "\n",
    "4. tune hiper parameters => done\n",
    "\n",
    "5. Ampliar el dataset con más NOK results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest clasifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_features=5,n_estimators=15 )\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_rf = rf.predict(X_train)\n",
    "\n",
    "print( np.unique( prediction_rf ) )\n",
    "\n",
    "print( accuracy_score(y_train, prediction_rf) )\n",
    "\n",
    " \n",
    "\n",
    "prob_y_4 = rf.predict_proba(X_train)\n",
    "prob_y_4 = [p[1] for p in prob_y_4]\n",
    "print( roc_auc_score(y_train, prob_y_4) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parece que estamos en las mismas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_rf_test=rf.predict(X_test)\n",
    "cm_rf = confusion_matrix(y_true=y_test,y_pred=prediction_rf_test)\n",
    "cm_plot_labels_rf=['no OK','OK']\n",
    "plot_confusion_matrix (cm_rf,cm_plot_labels_rf,title='Confusion matrix Random forest clasifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = rf.estimators_[5]\n",
    "feature_names=[]\n",
    "for col in dfX.columns:\n",
    "    feature_names.append(col)\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "# Export as dot file\n",
    "export_graphviz(estimator, out_file='tree2.dot', \n",
    "                feature_names = feature_names,\n",
    "                class_names = feature_names,\n",
    "                rounded = True, proportion = False, \n",
    "                precision = 2, filled = True)\n",
    "\n",
    "# Convert to png using system command (requires Graphviz)\n",
    "# from subprocess import call\n",
    "# call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'],shell=False)\n",
    "\n",
    "# Find the image on src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "call(['dot', '-Tpng', 'tree2.dot', '-o', 'tree.png', '-Gdpi=600'],shell=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1275a05ae34f0c315803f9d1758b76ed30a4d7265e5e486823be65aff6a6df27"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
